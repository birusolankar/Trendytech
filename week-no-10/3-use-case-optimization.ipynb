{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068b0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/itv015970/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e5f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = \"order_id long, order_date string, customer_id long, order_status string\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b4167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(order_schema) \\\n",
    ".load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3607da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e9c8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_schema = \"customerid long, customer_fname string, customer_lname string, username string, password string, address string, city string, state string, pincode long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdca6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(customers_schema) \\\n",
    ".load(\"/public/trendytech/retail_db/customersnew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98199b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "|customerid|customer_fname|customer_lname| username| password|             address|         city|state|pincode|\n",
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "|         1|       Richard|     Hernandez|XXXXXXXXX|XXXXXXXXX|  6303 Heather Plaza|  Brownsville|   TX|  78521|\n",
      "|         2|          Mary|       Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...|    Littleton|   CO|  80126|\n",
      "|         3|           Ann|         Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|       Caguas|   PR|    725|\n",
      "|         4|          Mary|         Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common|   San Marcos|   CA|  92069|\n",
      "|         5|        Robert|        Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|       Caguas|   PR|    725|\n",
      "|         6|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|3151 Sleepy Quail...|      Passaic|   NJ|   7055|\n",
      "|         7|       Melissa|        Wilcox|XXXXXXXXX|XXXXXXXXX|9453 High Concession|       Caguas|   PR|    725|\n",
      "|         8|         Megan|         Smith|XXXXXXXXX|XXXXXXXXX|3047 Foggy Forest...|     Lawrence|   MA|   1841|\n",
      "|         9|          Mary|         Perez|XXXXXXXXX|XXXXXXXXX| 3616 Quaking Street|       Caguas|   PR|    725|\n",
      "|        10|       Melissa|         Smith|XXXXXXXXX|XXXXXXXXX|8598 Harvest Beac...|     Stafford|   VA|  22554|\n",
      "|        11|          Mary|       Huffman|XXXXXXXXX|XXXXXXXXX|    3169 Stony Woods|       Caguas|   PR|    725|\n",
      "|        12|   Christopher|         Smith|XXXXXXXXX|XXXXXXXXX|5594 Jagged Ember...|  San Antonio|   TX|  78227|\n",
      "|        13|          Mary|       Baldwin|XXXXXXXXX|XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|   PR|    725|\n",
      "|        14|     Katherine|         Smith|XXXXXXXXX|XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|   CA|  90660|\n",
      "|        15|          Jane|          Luna|XXXXXXXXX|XXXXXXXXX|    673 Burning Glen|      Fontana|   CA|  92336|\n",
      "|        16|       Tiffany|         Smith|XXXXXXXXX|XXXXXXXXX|      6651 Iron Port|       Caguas|   PR|    725|\n",
      "|        17|          Mary|      Robinson|XXXXXXXXX|XXXXXXXXX|     1325 Noble Pike|       Taylor|   MI|  48180|\n",
      "|        18|        Robert|         Smith|XXXXXXXXX|XXXXXXXXX|2734 Hazy Butterf...|     Martinez|   CA|  94553|\n",
      "|        19|     Stephanie|      Mitchell|XXXXXXXXX|XXXXXXXXX|3543 Red Treasure...|       Caguas|   PR|    725|\n",
      "|        20|          Mary|         Ellis|XXXXXXXXX|XXXXXXXXX|      4703 Old Route|West New York|   NJ|   7093|\n",
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5a961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.join(customers_df.distinct(), orders_df.customer_id == customers_df.distinct().customerid, \"inner\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a80ef1",
   "metadata": {},
   "source": [
    "### 3 problems we have\n",
    "1. We have 200 shuffle partitions but only few of them have data\n",
    "2. when there is a dominating key, and there is a partition skew\n",
    "3. I want to join orders with of distinct customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174358c",
   "metadata": {},
   "source": [
    "### Solutiins to above 3 problems\n",
    "\n",
    "#AQE provides\n",
    "============\n",
    "1. dynamically coalesing the number of shuffle partitions\n",
    "2. dynamically handling partition skew\n",
    "3. dynamically switching join strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ba633",
   "metadata": {},
   "source": [
    "### number of records:\n",
    "\n",
    "1. size of the data\n",
    "2. min and max of each column\n",
    "3. count of number of occurences of each key\n",
    "4. if data is reduced during the join it dynamically switch the join strategies like broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f78aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
